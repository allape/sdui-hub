name: ollama
services:
  proxy:
    image: "tinyproxy"
    networks:
      - external
      - internal
    volumes:
      - ./tinyproxy.conf:/etc/tinyproxy/tinyproxy.conf
    extra_hosts:
      - "host.docker.internal:host-gateway"
  http:
    image: "caddy"
    networks:
      - external
      - internal
    depends_on:
      - app
      - webui
    ports:
      - 11434:11434
      - 11435:11435
    volumes:
      - ./caddy.ollama.Caddyfile:/etc/caddy/Caddyfile
    extra_hosts:
      - "host.docker.internal:host-gateway"
  app:
    image: "ollama/ollama"
    networks:
      - internal
    environment:
      - http_proxy=http://proxy:8118
      - https_proxy=http://proxy:8118
      - no_proxy=localhost,127.0.0.1,0.0.0.0,app,webui,proxy,::1
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    volumes:
      - ./ollama/app:/root/.ollama:z
  webui:
    image: "ghcr.io/open-webui/open-webui:main"
    links:
      - app
    networks:
      - internal
    depends_on:
      - app
    environment:
      - OLLAMA_BASE_URL=http://app:11434
      - http_proxy=http://proxy:8118
      - https_proxy=http://proxy:8118
      - no_proxy=localhost,127.0.0.1,app,0.0.0.0
    volumes:
      - ./ollama/webui:/app/backend/data:z

networks:
  external:
  internal:
    internal: true
